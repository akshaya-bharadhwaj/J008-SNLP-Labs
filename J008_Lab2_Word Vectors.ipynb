{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Using Pretrained Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pretrained word2vec-google-news-300 model\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'happy':\n",
      "  glad: 0.7408890724182129\n",
      "  pleased: 0.6632170677185059\n",
      "  ecstatic: 0.6626912355422974\n",
      "  overjoyed: 0.6599286794662476\n",
      "  thrilled: 0.6514049172401428\n",
      "  satisfied: 0.6437949538230896\n",
      "  proud: 0.636042058467865\n",
      "  delighted: 0.627237856388092\n",
      "  disappointed: 0.6269949674606323\n",
      "  excited: 0.6247665286064148\n",
      "\n",
      "Words similar to 'movie':\n",
      "  film: 0.8676770329475403\n",
      "  movies: 0.8013108372688293\n",
      "  films: 0.7363011837005615\n",
      "  moive: 0.6830360889434814\n",
      "  Movie: 0.6693680286407471\n",
      "  horror_flick: 0.6577848792076111\n",
      "  sequel: 0.6577793955802917\n",
      "  Guy_Ritchie_Revolver: 0.650975227355957\n",
      "  romantic_comedy: 0.6413198709487915\n",
      "  flick: 0.6321909427642822\n",
      "\n",
      "Words similar to 'actor':\n",
      "  actress: 0.7930010557174683\n",
      "  Actor: 0.7446156740188599\n",
      "  thesp: 0.6954971551895142\n",
      "  thespian: 0.6651668548583984\n",
      "  actors: 0.6519852876663208\n",
      "  funnyman: 0.635244607925415\n",
      "  comedian_Dom_DeLuise: 0.6245246529579163\n",
      "  entertainer: 0.6184110641479492\n",
      "  Shakespearean_actor: 0.6067742705345154\n",
      "  Oscarwinning: 0.6048368215560913\n",
      "\n",
      "Words similar to 'love':\n",
      "  loved: 0.6907791495323181\n",
      "  adore: 0.6816873550415039\n",
      "  loves: 0.661863386631012\n",
      "  passion: 0.6100708842277527\n",
      "  hate: 0.600395679473877\n",
      "  loving: 0.5886635780334473\n",
      "  Ilove: 0.5702950954437256\n",
      "  affection: 0.5664337873458862\n",
      "  undying_love: 0.5547304749488831\n",
      "  absolutely_adore: 0.5536840558052063\n",
      "\n",
      "Words similar to 'sad':\n",
      "  saddening: 0.7273085713386536\n",
      "  Sad: 0.6610826849937439\n",
      "  saddened: 0.6604382395744324\n",
      "  heartbreaking: 0.6573508381843567\n",
      "  disheartening: 0.6507317423820496\n",
      "  Meny_Friedman: 0.6487058401107788\n",
      "  parishioner_Pat_Patello: 0.6475860476493835\n",
      "  saddens_me: 0.6407119035720825\n",
      "  distressing: 0.6399092674255371\n",
      "  reminders_bobbing: 0.6357713341712952\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick 5 words and find similar words\n",
    "words = [\"happy\", \"movie\", \"actor\", \"love\", \"sad\"]\n",
    "similar_words = {word: model.most_similar(word) for word in words}\n",
    "\n",
    "# Display similar words\n",
    "for word, similar in similar_words.items():\n",
    "    print(f\"Words similar to '{word}':\")\n",
    "    for similar_word, similarity in similar:\n",
    "        print(f\"  {similar_word}: {similarity}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman = queen (similarity: 0.7118193507194519)\n",
      "Paris - France + Germany = Berlin (similarity: 0.7644002437591553)\n",
      "big - bigger + small = large (similarity: 0.6242177486419678)\n"
     ]
    }
   ],
   "source": [
    "# Analogies\n",
    "analogies = [\n",
    "    (\"king\", \"man\", \"woman\"),  # Expected result: 'queen'\n",
    "    (\"Paris\", \"France\", \"Germany\"),  # Expected result: 'Berlin'\n",
    "    (\"big\", \"bigger\", \"small\"),  # Expected result: 'smaller'\n",
    "]\n",
    "\n",
    "for a, b, c in analogies:\n",
    "    result = model.most_similar(positive=[a, c], negative=[b])\n",
    "    print(f\"{a} - {b} + {c} = {result[0][0]} (similarity: {result[0][1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Movie Review Sentiment Classifier Using Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the data\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove punctuation, special characters, and stopwords\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewers mentioned watching oz episode yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically theres family little boy jake thinks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                      cleaned_review  \n",
       "0  one reviewers mentioned watching oz episode yo...  \n",
       "1  wonderful little production filming technique ...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically theres family little boy jake thinks...  \n",
       "4  petter matteis love time money visually stunni...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Train Custom Models using Skip Gram and CBoW Vectors\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#Tokenize Reviews\n",
    "\n",
    "tokenized_reviews = [review.split() for review in df['cleaned_review']]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(tokenized_reviews, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Word2Vec models\n",
    "sg_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=1, workers=4, sg=1)  # Skip-gram\n",
    "cbow_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=1, workers=4, sg=0)  # CBOW\n",
    "\n",
    "# Get average word vectors for each review\n",
    "def get_average_word2vec(tokens_list, model, vector_size):\n",
    "    # Remove out-of-vocabulary words\n",
    "    tokens_list = [token for token in tokens_list if token in model.wv.index_to_key]\n",
    "    if not tokens_list:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(model.wv[tokens_list], axis=0)\n",
    "\n",
    "X_train_sg = [get_average_word2vec(review, sg_model, 100) for review in X_train]\n",
    "X_test_sg = [get_average_word2vec(review, sg_model, 100) for review in X_test]\n",
    "\n",
    "X_train_cbow = [get_average_word2vec(review, cbow_model, 100) for review in X_train]\n",
    "X_test_cbow = [get_average_word2vec(review, cbow_model, 100) for review in X_test]\n",
    "\n",
    "# Train logistic regression models\n",
    "lr_sg = LogisticRegression(max_iter=1000)\n",
    "lr_sg.fit(X_train_sg, y_train)\n",
    "y_pred_sg = lr_sg.predict(X_test_sg)\n",
    "accuracy_sg = accuracy_score(y_test, y_pred_sg)\n",
    "\n",
    "lr_cbow = LogisticRegression(max_iter=1000)\n",
    "lr_cbow.fit(X_train_cbow, y_train)\n",
    "y_pred_cbow = lr_cbow.predict(X_test_cbow)\n",
    "accuracy_cbow = accuracy_score(y_test, y_pred_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model Using Pretrained Word2Vec Vectors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_average_pretrained_word2vec(tokens_list, model, vector_size):\n",
    "    tokens_list = [token for token in tokens_list if token in model.key_to_index]\n",
    "    if not tokens_list:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean([model[token] for token in tokens_list], axis=0)\n",
    "\n",
    "X_train_pretrained = [get_average_pretrained_word2vec(review, model, 300) for review in X_train]\n",
    "X_test_pretrained = [get_average_pretrained_word2vec(review, model, 300) for review in X_test]\n",
    "\n",
    "# Train logistic regression model\n",
    "lr_pretrained = LogisticRegression(max_iter=1000)\n",
    "lr_pretrained.fit(X_train_pretrained, y_train)\n",
    "y_pred_pretrained = lr_pretrained.predict(X_test_pretrained)\n",
    "accuracy_pretrained = accuracy_score(y_test, y_pred_pretrained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy\n",
      "0            Skip-gram    0.8679\n",
      "1                 CBOW    0.8502\n",
      "2  Pretrained Word2Vec    0.8500\n"
     ]
    }
   ],
   "source": [
    "# Report the metrics\n",
    "metrics = {\n",
    "    \"Model\": [\"Skip-gram\", \"CBOW\", \"Pretrained Word2Vec\"],\n",
    "    \"Accuracy\": [accuracy_sg, accuracy_cbow, accuracy_pretrained]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference:\n",
    "\n",
    "1. The Skip-gram model has the highest accuracy. This could mean that the Skip-gram model was able to generate more informative and discriminative word embeddings.\n",
    "\n",
    "2. Despite being marginally less accurate, the CBOW model still demonstrates good performance, which means that it can also generate useful word embeddings.\n",
    "\n",
    "3. The performance of the pretrained model indicates that while it provides a strong baseline, custom training on the specific dataset (as done with Skip-gram and CBOW) can yield better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
